# -*- coding: utf-8 -*-
"""Copy of Weld_Defects_using_ResNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KExW2tQUT9GzVyz3JZr45xOF0SuDYy81
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'tig-aluminium-5083:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F63362%2F123140%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240301%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240301T053901Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80b7d67139cd5d23cdf4610033441b19f87cc7bd10b43a967b3273b894a1a0278b1e178869784c53c07fbefd15c1ad2e145d81f03c053108aaeecb7891578e9d097998506602108068f64693734e1c5c54afd1514796683089d4108c7c5f8f6a3395c7c2f3f51a6a852d38a2f8015ac746cfde106fe1b434c5252c19739109b4b0d29088be5c64f28a10f40df3685253b43dbe762da1801ba73a5bde4d683dbb71c55d2dc53e7a0118abd56d480cc6e998255344de4be8be924bba466e09a61820afe0d1db62faf2a451f81e81211b1b24b308e1479e974fa01ee95ea6a394278967d6a6c8caeb9fd45338827d5b25f845a8c7ab507b29f295f00221d2116194'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""NEW **CODE**"""

import json

with open('/kaggle/input/tig-aluminium-5083/al5083/train/train.json', 'r') as file:
    class_mapping = json.load(file)

import json

with open('/kaggle/input/tig-aluminium-5083/al5083/test/test.json', 'r') as file:
    class_mapping = json.load(file)

import os
import numpy as np
import keras
from keras.preprocessing.image import load_img, img_to_array

class CustomDataGenerator(keras.utils.Sequence):
    def __init__(self, mapping, batch_size=32, dim=(224,224), n_channels=3, n_classes=6, shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.mapping = mapping
        self.list_IDs = list(mapping.keys())
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        X, y = self.__data_generation(list_IDs_temp)
        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples'
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size), dtype=int)

        for i, ID in enumerate(list_IDs_temp):
            img_path = os.path.join('/kaggle/input/tig-aluminium-5083/al5083/train/', ID) # Adjust path as necessary
            img = load_img(img_path, target_size=self.dim)
            X[i,] = img_to_array(img) / 255.0
            y[i] = self.mapping[ID]

        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)

import os
import numpy as np
import keras
from keras.preprocessing.image import load_img, img_to_array

class CustomDataGenerator(keras.utils.Sequence):
    def __init__(self, mapping, batch_size=32, dim=(224,224), n_channels=3, n_classes=6, shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.mapping = mapping
        self.list_IDs = list(mapping.keys())
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        X, y = self.__data_generation(list_IDs_temp)
        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples'
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size), dtype=int)

        for i, ID in enumerate(list_IDs_temp):
            img_path = os.path.join('/kaggle/input/tig-aluminium-5083/al5083/test/', ID) # Adjust path as necessary
            img = load_img(img_path, target_size=self.dim)
            X[i,] = img_to_array(img) / 255.0
            y[i] = self.mapping[ID]

        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)

train_generator = CustomDataGenerator(mapping=class_mapping, batch_size=32, dim=(224,224), n_channels=3, n_classes=6, shuffle=True)

from keras.applications.resnet50 import ResNet50
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

# Load the ResNet50 model pre-trained weights
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(6, activation='softmax')(x)  # Assuming 6 classes

# This is the model we will train
model = Model(inputs=base_model.input, outputs=predictions)

# First: train only the top layers (which were randomly initialized)
for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model on the new data
history=model.fit_generator(generator=train_generator,  # Your custom train generator  # Your custom validation generator
                    steps_per_epoch=15,  # Adjust based on your dataset
                    validation_steps=50,  # Adjust based on your dataset
                    epochs=280)  # Adjust based on your needs

# Assuming class_mapping_test is your test dataset mapping loaded from a JSON file
test_generator = CustomDataGenerator(mapping=class_mapping, batch_size=32, dim=(224,224), n_channels=3, n_classes=6, shuffle=False)

# Evaluate the model
loss, accuracy = model.evaluate(test_generator)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")

import matplotlib.pyplot as plt

# Plot training & validation loss values
plt.figure(figsize=(10, 4))
plt.plot(history.history['loss'], label='Training loss')
if 'val_loss' in history.history:  # Check if validation loss is available
    plt.plot(history.history['val_loss'], label='Validation loss')
plt.title('Model Loss Evolution')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc='upper right')
plt.show()

# Plot training & validation accuracy values
plt.figure(figsize=(10, 4))
plt.plot(history.history['accuracy'], label='Training accuracy')
if 'val_accuracy' in history.history:  # Check if validation accuracy is available
    plt.plot(history.history['val_accuracy'], label='Validation accuracy', linestyle='-', alpha=0.5)
plt.title('Model Accuracy Evolution')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc='lower right')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

# Generate predictions for the test data
y_pred_prob = model.predict(test_generator)
y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels

# Get the true labels
y_true = np.array([class_mapping[ID] for ID in test_generator.list_IDs])

# Ensure lengths match
min_length = min(len(y_true), len(y_pred))
y_true = y_true[:min_length]
y_pred = y_pred[:min_length]

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred)

# Define class names
class_names = ['good weld', 'burn through', 'contamination', 'lack of fusion', 'misalignment', 'lack of penetration']  # Adjust based on your classes

# Plot confusion matrix
plt.figure(figsize=(8, 6))  # Set the figure size
sns.heatmap(conf_matrix, annot=False, cmap='viridis', cbar=True, xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()